This project focuses on converting sign language into audible speech to facilitate communication between the hearing and speech-impaired communities. Using machine learning, it recognizes hand gestures and translates them into speech. The model is trained with images of sign language gestures via Googleâ€™s Teachable Machine and implemented using Keras for deep learning. Real-time gesture recognition is enabled through a webcam, processed with OpenCV, and the recognized gestures are converted into spoken words. The project includes a dataset of hand gesture images, a trained Keras model, and a Python script to integrate the model with speech output.
